{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is our Embedding model, designed to compete with our TF-IDF model. We will take the more accurate model for our final product, but we wanted to explore both options and test them against each other."
      ],
      "metadata": {
        "id": "hxALLDGzc8Zr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyo-yL95LUVC",
        "outputId": "22aa786a-518e-4c23-b117-c7a1f76e984b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"billsTrain.csv\")\n",
        "if df.isnull().values.any():\n",
        "  df = df.dropna()\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wz0Irme5LjXA",
        "outputId": "9e5ae013-deb0-43e2-cfb6-1e2dd2b9d12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Title  Major  Minor\n",
              "1880  A bill to establish a new program of health ca...    3.0  301.0\n",
              "1881              An Act to provide for pension reform.    5.0  503.0\n",
              "1882  A bill to provide for the regulation of surfac...    8.0  805.0\n",
              "1883  A bill to provide that meetings of Government ...    2.0  208.0\n",
              "1884  A bill to amend the Internal Revenue Code of 1...    6.0  601.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9667cbff-dd45-45e9-8054-9f349cab77e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Major</th>\n",
              "      <th>Minor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>A bill to establish a new program of health ca...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>301.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1881</th>\n",
              "      <td>An Act to provide for pension reform.</td>\n",
              "      <td>5.0</td>\n",
              "      <td>503.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1882</th>\n",
              "      <td>A bill to provide for the regulation of surfac...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>805.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1883</th>\n",
              "      <td>A bill to provide that meetings of Government ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>208.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1884</th>\n",
              "      <td>A bill to amend the Internal Revenue Code of 1...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>601.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9667cbff-dd45-45e9-8054-9f349cab77e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9667cbff-dd45-45e9-8054-9f349cab77e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9667cbff-dd45-45e9-8054-9f349cab77e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-08fee75b-aac4-4bfe-90c9-0591e6d14df2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08fee75b-aac4-4bfe-90c9-0591e6d14df2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-08fee75b-aac4-4bfe-90c9-0591e6d14df2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 59757,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33146,\n        \"samples\": [\n          \"A bill to authorize the Secretary of Commerce to engage in certain small business export expansion activities.\",\n          \"A bill to amend the Clean Air Act to establish certain motor vehicle emission standards.\",\n          \"A bill to reorganize the governmental structure of the District of Columbia, to provide a charter for local government in the District of Columbia subject to acceptance by a majority of the registered qualified electors in the District of Columbia, to delegate certain legislative powers to the local government, to implement certain recommendations of the the Commission on the Organization of the government of the District of Columbia, and for other purposes.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Major\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.120502238154405,\n        \"min\": 1.0,\n        \"max\": 99.0,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          3.0,\n          10.0,\n          19.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Minor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2435.0520149643608,\n        \"min\": 100.0,\n        \"max\": 9999.0,\n        \"num_unique_values\": 208,\n        \"samples\": [\n          206.0,\n          600.0,\n          701.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = df['Title']\n",
        "labelsMajor = df['Major']\n",
        "labelsMinor = df['Minor']"
      ],
      "metadata": {
        "id": "1bfzHXt2MNmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each title individually to remove punctuation\n",
        "inputs = inputs.str.replace(r'\\.(?!\\d)', '. ')\n",
        "\n",
        "processed_titles = []\n",
        "for title in inputs:\n",
        "    clean_title = ''.join([char for char in title if char not in punctuation])\n",
        "    processed_titles.append(clean_title)\n",
        "\n",
        "# Combine all processed words into a single list for vocabulary building\n",
        "# This ensures the vocabulary is built from all unique words across all titles\n",
        "all_text_for_vocab = ' '.join(processed_titles)\n",
        "words = all_text_for_vocab.split()\n",
        "words[:50]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LOktRX_3L4bL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2c19f8-3084-4f9a-b025-c6ad94f1375b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'bill',\n",
              " 'to',\n",
              " 'establish',\n",
              " 'a',\n",
              " 'new',\n",
              " 'program',\n",
              " 'of',\n",
              " 'health',\n",
              " 'care',\n",
              " 'delivery',\n",
              " 'and',\n",
              " 'comprehensive',\n",
              " 'health',\n",
              " 'care',\n",
              " 'delivery',\n",
              " 'and',\n",
              " 'comprehensive',\n",
              " 'health',\n",
              " 'care',\n",
              " 'benefits',\n",
              " 'including',\n",
              " 'catastrophic',\n",
              " 'coverage',\n",
              " 'to',\n",
              " 'be',\n",
              " 'available',\n",
              " 'to',\n",
              " 'aged',\n",
              " 'persons',\n",
              " 'and',\n",
              " 'to',\n",
              " 'employed',\n",
              " 'unemployed',\n",
              " 'and',\n",
              " 'lowincome',\n",
              " 'individuals',\n",
              " 'at',\n",
              " 'a',\n",
              " 'cost',\n",
              " 'related',\n",
              " 'to',\n",
              " 'their',\n",
              " 'income',\n",
              " 'An',\n",
              " 'Act',\n",
              " 'to',\n",
              " 'provide',\n",
              " 'for',\n",
              " 'pension']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab))) # should ~ 74000+\n",
        "print()\n",
        "vocab[-10:]\n",
        "## Build a dictionary that maps words to integers\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} # here 1 means the index starts from 1"
      ],
      "metadata": {
        "id": "HvPafku5Orzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41d2435-9889-4c5d-c906-46394efe6f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words:  21047\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_ints = []\n",
        "for title_text in processed_titles: # Iterate over the correctly processed titles\n",
        "  if title_text.strip(): # Check if the cleaned title is not empty\n",
        "    words_ints.append([vocab_to_int[word] for word in title_text.split() if word in vocab_to_int])\n",
        "  else:\n",
        "    words_ints.append([]) # Append an empty list for titles that became empty after cleaning\n",
        "\n",
        "type(words_ints)"
      ],
      "metadata": {
        "id": "or2GWgjSbALr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3228225-8259-45c2-bfb9-42fb8b59f90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define non_zero_idx by filtering out empty sequences from words_ints\n",
        "non_zero_idx = [i for i, seq in enumerate(words_ints) if len(seq) > 0]\n",
        "\n",
        "words_ints = [words_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([labelsMajor.iloc[ii] for ii in non_zero_idx])\n",
        "print('Number of Bills after removing outliers: ', len(words_ints))"
      ],
      "metadata": {
        "id": "64imp-E8APgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b082c4ec-5c18-42c3-9398-9777cb67ac0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Bills after removing outliers:  59757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now cleaned our inputs. We removed bills with N/As, removed punctuation, and created a vocabulary list of words."
      ],
      "metadata": {
        "id": "QKPwT_XKq8VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(words_ints, seq_length):\n",
        "  features = np.zeros((len(words_ints), seq_length), dtype=int)\n",
        "  for i, row in enumerate(words_ints):\n",
        "    if len(row) > 0: # Only assign if the row is not empty\n",
        "      to_assign = np.array(row)[:seq_length]\n",
        "      features[i, -len(to_assign):] = to_assign\n",
        "  return features"
      ],
      "metadata": {
        "id": "OyS2jzwM-QQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 50\n",
        "features = pad_features(words_ints, seq_length=seq_length)"
      ],
      "metadata": {
        "id": "rXAf3wLV-hv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vast majority of bills are shorter than 50 words, but we padded it to 50 because some observed entries are 20 or 30-odd words long. There may be longer bills too, and 50 seemed relatively safe."
      ],
      "metadata": {
        "id": "P4q1p-derLZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "AeDAKrUMLxll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de18adf-4436-49b1-e3b5-745eac6052c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    1,   60,   45],\n",
              "       [   0,    0,    0, ...,    7,  140,  510],\n",
              "       [   0,    0,    0, ...,    7,   21,   22],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  935,    9,  878],\n",
              "       [   0,    0,    0, ...,    7,   21,   22],\n",
              "       [   0,    0,    0, ...,    6, 3427,  874]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labelsMajor.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2iu0YYUTsKdO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "3c1a8959-d131-4569-ef9f-053228547f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1880    3.0\n",
              "1881    5.0\n",
              "1882    8.0\n",
              "1883    2.0\n",
              "1884    6.0\n",
              "Name: Major, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Major</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1881</th>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1882</th>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1883</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1884</th>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "# We now split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "\"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "\"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "metadata": {
        "id": "Ce9UAR7gAmR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca6e25f-9e7a-4752-9155-36f11ce1570b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(47805, 50) \n",
            "Validation set: \t(5976, 50) \n",
            "Test set: \t\t(5976, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that's the data prep. Most of it is very similar to what we did for Transformers, since that's what I based it off of..."
      ],
      "metadata": {
        "id": "cl4tEeOVBHCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "_Xi3TQZTA7UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "# dataloaders\n",
        "batch_size = 64\n",
        "# make sure we SHUFFLE that training data\n",
        "# drop_last=True will drop the last batch if the size is less than the given batch_size\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XAziVrKOBNRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7177a459-9e0b-44c6-93f2-e9fda6f5f963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input size:  torch.Size([64, 50])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,   424,   234,   874],\n",
            "        [    0,     0,     0,  ..., 15919, 15920,  1543],\n",
            "        [    0,     0,     0,  ...,  2371,    27,   466],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,    18,   258,    61],\n",
            "        [    0,     0,     0,  ...,   362,     1,   187],\n",
            "        [    0,     0,     0,  ...,    39,    17,   773]])\n",
            "\n",
            "Sample label size:  torch.Size([64])\n",
            "Sample label: \n",
            " tensor([ 1., 99., 21.,  9.,  6., 99.,  8.,  5., 99.,  3., 10., 20.,  4., 10.,\n",
            "         6.,  7.,  9.,  5.,  8., 20., 15., 15., 20., 12., 15., 20., 99., 18.,\n",
            "        99., 12., 12.,  7., 13., 10., 12.,  1., 12.,  7.,  8.,  8., 15., 13.,\n",
            "         8., 12., 13.,  1.,  7., 99., 14.,  7., 20., 16., 12.,  8., 18., 20.,\n",
            "        13., 16.,  4.,  4., 16., 20., 20., 20.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "  print('Training on GPU.')\n",
        "else:\n",
        "  print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "id": "WOHMKgoVBfo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17037bdc-da69-4df8-caac-c27b314fae18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, sequence_length, input_dim, output_dim):\n",
        "    super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "    # Use standard nn.Embedding for token embeddings, with padding_idx=0\n",
        "    self.token_embeddings = nn.Embedding(input_dim, output_dim, padding_idx=0)\n",
        "\n",
        "    # Use standard nn.Embedding for positional embeddings\n",
        "    # The weights will be randomly initialized by default\n",
        "    self.position_embeddings = nn.Embedding(sequence_length, output_dim)\n",
        "\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    device = inputs.device\n",
        "    # Create position indices for the entire sequence length\n",
        "    positions = torch.arange(0, self.sequence_length).unsqueeze(0).to(device)\n",
        "\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "\n",
        "    # Add token embeddings and positional embeddings\n",
        "    embedded_combined = embedded_tokens + embedded_positions\n",
        "    return embedded_combined"
      ],
      "metadata": {
        "id": "MNhOEFvUBs_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "embed_dim = 512\n",
        "# Define the embedding layer with positional information\n",
        "input_embed=PositionalEmbedding(seq_length, vocab_size, embed_dim)\n",
        "input_embed\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "sample_x.shape, sample_y.shape\n",
        "sample_emd=input_embed(sample_x)"
      ],
      "metadata": {
        "id": "3ZYXr8MLCgRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = 23\n",
        "embedding_dim = 1024\n",
        "hidden_dim = 512\n",
        "n_layers = 3\n",
        "\n",
        "class PredictRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform prediction of categories.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(PredictRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #TODO: add embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # linear layer - removed sigmoid\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "\n",
        "        x = x.long()\n",
        "\n",
        "        #TODO: compute embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        lstm_out = lstm_out[:, -1, :] # getting the last time step output\n",
        "\n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out) # output raw logits\n",
        "\n",
        "        # return last output and hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden\n",
        "\n",
        "net = PredictRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "id": "jms1fDdx6498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c45693-61b8-4211-9666-7e0a6abb1bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PredictRNN(\n",
            "  (embedding): Embedding(21048, 1024)\n",
            "  (lstm): LSTM(1024, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=23, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "sample_x.shape, sample_y.shape"
      ],
      "metadata": {
        "id": "RfAIr38AMN_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e791b7-0322-4035-8a04-e5589365d59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 50]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_emd=input_embed(sample_x)\n",
        "sample_emd.shape"
      ],
      "metadata": {
        "id": "bosS6aonMWZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323b0068-210c-459d-be18-b574107c344f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "#Input and output both have size (batch_size, seq_len, embed_dim)\n",
        "    self.attention = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
        "#TODO: define a two-layer Feed-forward network with hidden layer size dense_dim and output layer size embed_\n",
        "    self.dense_proj = nn.Sequential(\n",
        "    nn.Linear(embed_dim, dense_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(dense_dim, embed_dim)\n",
        "    )\n",
        "#TODO: define two layer normalization layers\n",
        "    self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
        "    self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    attention_output, _  = self.attention(inputs, inputs, inputs)\n",
        "    proj_input = inputs + attention_output\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "7xunA_bWMx_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, num_heads, dense_dim, sequence_length):\n",
        "    super(TransformerEncoderModel, self).__init__()\n",
        "    self.embedding = PositionalEmbedding(sequence_length,vocab_size,embed_dim)\n",
        "    self.transformer_encoder =TransformerEncoder(embed_dim,dense_dim,num_heads)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.fc = nn.Linear(embed_dim, 22)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "#TODO: compute the transformer output\n",
        "    x = self.transformer_encoder(x) # x has shape (Batch, Seq_Len, Embed_dim)\n",
        "    x,_ = torch.max(x, dim=1) # x has shape (Batch, Embd_dim)\n",
        "    x = self.dropout(x) # pass dropout layer\n",
        "    x = self.fc(x) # pass a linear layer\n",
        "    return self.sigmoid(x) # pass sigmoid activation"
      ],
      "metadata": {
        "id": "WGhgNl6SNTrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_heads = 2\n",
        "dense_dim = 1024\n",
        "\n",
        "num_classes = int(np.max(encoded_labels))\n",
        "\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, num_heads, dense_dim, sequence_length, num_classes):\n",
        "    super(TransformerEncoderModel, self).__init__()\n",
        "    self.embedding = PositionalEmbedding(sequence_length,vocab_size,embed_dim)\n",
        "    self.transformer_encoder =TransformerEncoder(embed_dim,dense_dim,num_heads)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.fc = nn.Linear(embed_dim, num_classes) # Correct output size\n",
        "    # self.sigmoid = nn.Sigmoid() # Removed sigmoid for CrossEntropyLoss\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.transformer_encoder(x) # x has shape (Batch, Seq_Len, Embed_dim)\n",
        "    x,_ = torch.max(x, dim=1) # x has shape (Batch, Embd_dim)\n",
        "    x = self.dropout(x) # pass dropout layer\n",
        "    x = self.fc(x) # pass a linear layer\n",
        "    return x # Return logits directly\n",
        "\n",
        "model = TransformerEncoderModel(vocab_size, embed_dim, num_heads, dense_dim, seq_length, num_classes)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 35"
      ],
      "metadata": {
        "id": "GMBilGRbLaGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcf7e78-2217-417f-d1d2-dc3416737a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerEncoderModel(\n",
            "  (embedding): PositionalEmbedding(\n",
            "    (token_embeddings): Embedding(21048, 512, padding_idx=0)\n",
            "    (position_embeddings): Embedding(50, 512)\n",
            "  )\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (attention): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "    )\n",
            "    (dense_proj): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    )\n",
            "    (layernorm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (layernorm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=99, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set the number of epochs to 35, but in a real testing environment this would probably be lower since we seem to start overfitting around epoch 15."
      ],
      "metadata": {
        "id": "5lOg1iA4gk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, num_epochs=10, train_on_gpu=False):\n",
        "  # Move model to GPU if available before optimizer initialization\n",
        "  if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "      if(train_on_gpu):\n",
        "        inputs,labels = inputs.cuda(), labels.cuda()\n",
        "      # Adjust labels to be 0-indexed for CrossEntropyLoss if they are 1-indexed\n",
        "      # and ensure they are Long type.\n",
        "      labels = (labels - 1).long()\n",
        "\n",
        "      outputs = model(inputs) # Model now outputs (batch_size, num_classes) logits, no squeeze(1)\n",
        "      loss = criterion(outputs, labels) # Pass Long type labels\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "      ### Validation Loop ###\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in valid_loader:\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels= inputs.cuda(), labels.cuda()\n",
        "        labels = (labels - 1).long() # Adjust labels for validation data too\n",
        "\n",
        "        outputs = model(inputs) # Get logits\n",
        "        _, predicted = torch.max(outputs, 1) # Get predicted class index for multi-class\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        val_acc = 100 * correct / total\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "t5bzxn9GEnmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, valid_loader, num_epochs, train_on_gpu)"
      ],
      "metadata": {
        "id": "J2eYizCVPQau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b6b789-5aad-4c77-dccd-e6fdca05ceb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35, Training Loss: 1.5320034976299584\n",
            "Epoch 1/35, Validation Accuracy: 67.36%\n",
            "Epoch 2/35, Training Loss: 0.738849476459199\n",
            "Epoch 2/35, Validation Accuracy: 73.81%\n",
            "Epoch 3/35, Training Loss: 0.5400345933001898\n",
            "Epoch 3/35, Validation Accuracy: 76.81%\n",
            "Epoch 4/35, Training Loss: 0.4569833791807254\n",
            "Epoch 4/35, Validation Accuracy: 78.02%\n",
            "Epoch 5/35, Training Loss: 0.37111592278803324\n",
            "Epoch 5/35, Validation Accuracy: 78.78%\n",
            "Epoch 6/35, Training Loss: 0.3364115332630301\n",
            "Epoch 6/35, Validation Accuracy: 78.81%\n",
            "Epoch 7/35, Training Loss: 0.30540328973220116\n",
            "Epoch 7/35, Validation Accuracy: 78.18%\n",
            "Epoch 8/35, Training Loss: 0.2926688487004818\n",
            "Epoch 8/35, Validation Accuracy: 78.48%\n",
            "Epoch 9/35, Training Loss: 0.25770867520059404\n",
            "Epoch 9/35, Validation Accuracy: 79.10%\n",
            "Epoch 10/35, Training Loss: 0.22520568766868387\n",
            "Epoch 10/35, Validation Accuracy: 78.85%\n",
            "Epoch 11/35, Training Loss: 0.21289917920284154\n",
            "Epoch 11/35, Validation Accuracy: 79.89%\n",
            "Epoch 12/35, Training Loss: 0.20538652270293267\n",
            "Epoch 12/35, Validation Accuracy: 80.41%\n",
            "Epoch 13/35, Training Loss: 0.18456024481807573\n",
            "Epoch 13/35, Validation Accuracy: 79.99%\n",
            "Epoch 14/35, Training Loss: 0.1759673198560208\n",
            "Epoch 14/35, Validation Accuracy: 80.41%\n",
            "Epoch 15/35, Training Loss: 0.17025568977687458\n",
            "Epoch 15/35, Validation Accuracy: 77.79%\n",
            "Epoch 16/35, Training Loss: 0.16228865687023777\n",
            "Epoch 16/35, Validation Accuracy: 79.64%\n",
            "Epoch 17/35, Training Loss: 0.153271305042721\n",
            "Epoch 17/35, Validation Accuracy: 78.49%\n",
            "Epoch 18/35, Training Loss: 0.14488857772560845\n",
            "Epoch 18/35, Validation Accuracy: 79.33%\n",
            "Epoch 19/35, Training Loss: 0.1435007186838733\n",
            "Epoch 19/35, Validation Accuracy: 79.20%\n",
            "Epoch 20/35, Training Loss: 0.1270193358416352\n",
            "Epoch 20/35, Validation Accuracy: 79.62%\n",
            "Epoch 21/35, Training Loss: 0.12691053731152263\n",
            "Epoch 21/35, Validation Accuracy: 79.25%\n",
            "Epoch 22/35, Training Loss: 0.12432489449647692\n",
            "Epoch 22/35, Validation Accuracy: 80.02%\n",
            "Epoch 23/35, Training Loss: 0.12642037755767277\n",
            "Epoch 23/35, Validation Accuracy: 79.18%\n",
            "Epoch 24/35, Training Loss: 0.1225653012462919\n",
            "Epoch 24/35, Validation Accuracy: 78.70%\n",
            "Epoch 25/35, Training Loss: 0.11437403870778952\n",
            "Epoch 25/35, Validation Accuracy: 79.97%\n",
            "Epoch 26/35, Training Loss: 0.11690407514761904\n",
            "Epoch 26/35, Validation Accuracy: 78.71%\n",
            "Epoch 27/35, Training Loss: 0.10578718039815142\n",
            "Epoch 27/35, Validation Accuracy: 79.12%\n",
            "Epoch 28/35, Training Loss: 0.10359103534740194\n",
            "Epoch 28/35, Validation Accuracy: 78.18%\n",
            "Epoch 29/35, Training Loss: 0.11356805346596577\n",
            "Epoch 29/35, Validation Accuracy: 79.55%\n",
            "Epoch 30/35, Training Loss: 0.10434321542120671\n",
            "Epoch 30/35, Validation Accuracy: 79.12%\n",
            "Epoch 31/35, Training Loss: 0.09981380696857882\n",
            "Epoch 31/35, Validation Accuracy: 78.63%\n",
            "Epoch 32/35, Training Loss: 0.10191164795876088\n",
            "Epoch 32/35, Validation Accuracy: 78.70%\n",
            "Epoch 33/35, Training Loss: 0.102905464980966\n",
            "Epoch 33/35, Validation Accuracy: 78.86%\n",
            "Epoch 34/35, Training Loss: 0.10045819138501329\n",
            "Epoch 34/35, Validation Accuracy: 79.02%\n",
            "Epoch 35/35, Training Loss: 0.10238998028107647\n",
            "Epoch 35/35, Validation Accuracy: 79.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried modifying some hyperparameters in our testing, such as num_heads, embedding_dim, and epoches, but we never really got above mid-80s % accuracy. We present this version because it is the latest one we have."
      ],
      "metadata": {
        "id": "oew0cBNvuVUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(text, model, vocab_to_int, seq_length, train_on_gpu):\n",
        "    # 1. Preprocess the input text\n",
        "    clean_text = ''.join([char for char in text if char not in punctuation])\n",
        "    words = clean_text.split()\n",
        "\n",
        "    # Convert words to integers, handling out-of-vocabulary words by skipping them\n",
        "    text_ints = [vocab_to_int[word] for word in words if word in vocab_to_int]\n",
        "\n",
        "    if not text_ints:\n",
        "        print(\"No known words in the input text after preprocessing.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Pad the sequence\n",
        "    # The pad_features function expects a list of lists, so wrap text_ints\n",
        "    padded_features = pad_features([text_ints], seq_length=seq_length)\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    input_tensor = torch.from_numpy(padded_features).long()\n",
        "\n",
        "    # Move to GPU if available\n",
        "    if train_on_gpu:\n",
        "        input_tensor = input_tensor.cuda()\n",
        "\n",
        "    # 3. Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # 4. Perform inference\n",
        "    with torch.no_grad():\n",
        "        output_logits = model(input_tensor)\n",
        "\n",
        "    # 5. Interpret the output\n",
        "    # Apply softmax to get probabilities and get the predicted class index\n",
        "    probabilities = torch.softmax(output_logits, dim=1)\n",
        "    _, predicted_index = torch.max(probabilities, 1)\n",
        "\n",
        "    # Adjust the predicted index back to the original 1-indexed label\n",
        "    predicted_label = predicted_index.item() + 1\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Sample text to test the model\n",
        "sample_bill_title = \"An Act to Protect Hawai'is Coastal Waters from Overfishing\"\n",
        "\n",
        "# Get prediction\n",
        "predicted_major_category = predict_category(\n",
        "    sample_bill_title, model, vocab_to_int, seq_length, train_on_gpu\n",
        ")\n",
        "\n",
        "if predicted_major_category is not None:\n",
        "    print(f\"Sample Bill Title: {sample_bill_title}\")\n",
        "    print(f\"Predicted Major Category: {predicted_major_category}\")\n",
        "\n",
        "# Test with another example (from df for comparison)\n",
        "sample_from_df = df['Title'].iloc[0]\n",
        "original_label = labelsMajor.iloc[0]\n",
        "predicted_from_df = predict_category(\n",
        "    sample_from_df, model, vocab_to_int, seq_length, train_on_gpu\n",
        ")\n",
        "\n",
        "if predicted_from_df is not None:\n",
        "    print(f\"\\nSample from Dataset: {sample_from_df}\")\n",
        "    print(f\"Original Major Category: {original_label}\")\n",
        "    print(f\"Predicted Major Category: {predicted_from_df}\")"
      ],
      "metadata": {
        "id": "0WQRJzX5JSg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08bb647-73b0-4d25-dc01-9f0811c38f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Bill Title: An Act to Protect Hawai'is Coastal Waters from Overfishing\n",
            "Predicted Major Category: 4\n",
            "\n",
            "Sample from Dataset: A bill to establish a new program of health care delivery and comprehensive health care delivery and comprehensive health care benefits (including catastrophic coverage), to be available to aged persons, and to employed, unemployed, and low-income individuals, at a cost related to their income.\n",
            "Original Major Category: 3.0\n",
            "Predicted Major Category: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of comparison with the other models, notably TF-IDF, we wanted to sample some errors and evaluate what they were like."
      ],
      "metadata": {
        "id": "rltYq5pNukMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mismatched_records = []\n",
        "count_mismatches = 0\n",
        "max_mismatches = 30\n",
        "\n",
        "# Calculate the starting index in the original filtered data for the test set\n",
        "# This maps the 0-indexed test_x/test_y indices back to the original `features` array indices\n",
        "start_index_for_test_set_in_filtered_data = split_idx + test_idx\n",
        "\n",
        "print(\"Searching for mismatched predictions in the test set...\")\n",
        "for i in range(len(test_x)):\n",
        "    if count_mismatches >= max_mismatches:\n",
        "        break\n",
        "\n",
        "    # Get the actual original index from df for the current test sample\n",
        "    # This uses the `non_zero_idx` to map back to the original df row index\n",
        "    original_df_index_for_current_test_sample = non_zero_idx[start_index_for_test_set_in_filtered_data + i]\n",
        "\n",
        "    # Get the original title from the dataframe using the mapped index\n",
        "    original_title = df['Title'].iloc[original_df_index_for_current_test_sample]\n",
        "\n",
        "    # Get the true major category (these are already 1-indexed from encoded_labels)\n",
        "    true_major_category = test_y[i]\n",
        "\n",
        "    # Predict the category using the defined function\n",
        "    predicted_major_category = predict_category(\n",
        "        original_title, model, vocab_to_int, seq_length, train_on_gpu\n",
        "    )\n",
        "\n",
        "    # Compare prediction with true label\n",
        "    # predict_category returns 1-indexed labels, and true_major_category is also 1-indexed\n",
        "    if predicted_major_category is not None and predicted_major_category != true_major_category:\n",
        "        mismatched_records.append({\n",
        "            \"Original Title\": original_title,\n",
        "            \"Original Major Category\": int(true_major_category),\n",
        "            \"Predicted Major Category\": int(predicted_major_category)\n",
        "        })\n",
        "        count_mismatches += 1\n",
        "\n",
        "if mismatched_records:\n",
        "    print(f\"\\nFound {len(mismatched_records)} records where prediction did not match the original major category:\")\n",
        "    for record in mismatched_records:\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"Original Title: {record['Original Title']}\")\n",
        "        print(f\"Original Major Category: {record['Original Major Category']}\")\n",
        "        print(f\"Predicted Major Category: {record['Predicted Major Category']}\")\n",
        "else:\n",
        "    print(\"\\nNo mismatches found in the processed test samples, or less than 10 records were processed.\")\n"
      ],
      "metadata": {
        "id": "PSUm-ytSkcsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8056fa4-5aeb-42ab-fda3-884e5fb1cf4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for mismatched predictions in the test set...\n",
            "\n",
            "Found 30 records where prediction did not match the original major category:\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Public Health Service Act to provide for revision of the National Institute on Aging.\n",
            "Original Major Category: 13\n",
            "Predicted Major Category: 3\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to defer from income certain amounts deferred pursuant to State or local public employee deferred compensation plans.\n",
            "Original Major Category: 5\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to authorize the establishment of an international emergency wheat reserve, and for other related purposes.\n",
            "Original Major Category: 4\n",
            "Predicted Major Category: 18\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend title XIX of the Social Security Act to provide that certain handicapped individuals shall be eligible for medical assistance.\n",
            "Original Major Category: 3\n",
            "Predicted Major Category: 13\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to require certain Federal agencies to analyze complaints regarding their delivery of services and benefits and to report their findings to Congress.\n",
            "Original Major Category: 20\n",
            "Predicted Major Category: 13\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to regulate the trapping of mammals and birds on Federal lands, and for other purposes.\n",
            "Original Major Category: 21\n",
            "Predicted Major Category: 7\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to authorize appropriations for fiscal year 1979 for intelligence and intelligence-related activities of the United States Government, the Intelligence Community Staff, the Central Intelligence Agency Retirement and Disability System, and for other purposes.\n",
            "Original Major Category: 16\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Internal Revenue Code of 1954 to provide rules for the tax treatment of employees under certain profit sharing plans.\n",
            "Original Major Category: 5\n",
            "Predicted Major Category: 1\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to transfer certain real property of the United States to the District of Columbia Redevelopment Land Agency.\n",
            "Original Major Category: 20\n",
            "Predicted Major Category: 21\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to provide for a comparison study of the costs and other factors associated with the establishment of reimbursement guidelines for respiratory therapy.\n",
            "Original Major Category: 3\n",
            "Predicted Major Category: 18\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to extend countercyclical revenue sharing for three years and to revise the unemployment percentage required for authorization of such program.\n",
            "Original Major Category: 20\n",
            "Predicted Major Category: 5\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to encourage broader utilization of the condominium form of homeownership, to provide minimum national standards for disclosure and consumer protection for condominium purchasers and owners and tenants in condominium conversions, to encourage States to establish similar standards, to correct abusive use of long-term leasing of recreation and other condominium-related facilities, and for other purposes.\n",
            "Original Major Category: 14\n",
            "Predicted Major Category: 15\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend title 38 of the United States Code to provide that the Veterans' Administration will not deny financing assistance for the purchase of residential property solely because the property is located in an area identified by a Federal agency as an area subject to the highest noise level resulting from the operation of aircraft at a nearby civilian or military airport.\n",
            "Original Major Category: 14\n",
            "Predicted Major Category: 16\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend title XVIII of the Social Security Act to authorize payment under the supplementary medical insurance program for the cutting and removal of corns, warts and calluses and the reduction of club nails.\n",
            "Original Major Category: 3\n",
            "Predicted Major Category: 13\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to defer from income certain amounts deferred pursuant to State or local public employee deferred compensation plans.\n",
            "Original Major Category: 5\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Internal Revenue Code of 1954 to allow a credit against tax for the cost of removal of trees required by the United States or a State or local government to be removed to prevent the spread of a disease caused by pests.\n",
            "Original Major Category: 4\n",
            "Predicted Major Category: 3\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to allow the President to waive certain prohibitions on the furnishing of food assistance to foreign countries if he determines that such assistance would help needy people.\n",
            "Original Major Category: 19\n",
            "Predicted Major Category: 13\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend section 541 of title 28, United States Code, to provide for merit selection of United States attorneys.\n",
            "Original Major Category: 12\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Truth in Lending Act, the Rules of the House of Representatives, and title 13 of the United States Code to reduce the amount of paperwork required by persons subject to Federal regulations.\n",
            "Original Major Category: 15\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Internal Revenue Code of 1954 to suspend the imposition of interest on deficiencies of income tax which result from erroneous assistance given by the Internal Revenue Service, and for other purposes.\n",
            "Original Major Category: 20\n",
            "Predicted Major Category: 1\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Internal Revenue Code of 1954 to increase the limitations on the deduction of expenditures by farmers for clearing land.\n",
            "Original Major Category: 4\n",
            "Predicted Major Category: 21\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to regulate the trapping of mammals and birds on Federal lands, and for other purposes.\n",
            "Original Major Category: 21\n",
            "Predicted Major Category: 7\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to provide for reimbursement to States experiencing high rates of insured unemployment.\n",
            "Original Major Category: 13\n",
            "Predicted Major Category: 5\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to permit a church plan to continue after 1982 to provide benefits for employees of organizations controlled by or associated with the church and to make certain clarifying amendments to the definition of church plans.\n",
            "Original Major Category: 2\n",
            "Predicted Major Category: 5\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to reinstate the tax treatment with respect to annuity contracts with reserves based on a segregated asset account as they existed prior to issuance of Revenue Ruling 77-85.\n",
            "Original Major Category: 1\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Internal Revenue Code of 1954 to clarify standards for determining status of individuals for employment tax purposes.\n",
            "Original Major Category: 5\n",
            "Predicted Major Category: 1\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to limit the validity of passports issued to Federal officers, employees, and their dependents, for use in their official duties, to the period of the officer's or employee's official status.\n",
            "Original Major Category: 19\n",
            "Predicted Major Category: 20\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to amend the Internal Revenue Code of 1954 and the Social Security Act to provide an exemption from coverage under the social security program, through a tax refund procedure, for employees who are members of religious faiths which oppose participation in such program, and to provide a similar exemption on a current basis (pursuant to waiver certificates filed in advance) for employers engaged in farming and their employees in cases where both are members of such faiths; and to make the existing exemption for self-employed members of such faiths available to certain additional individuals.\n",
            "Original Major Category: 2\n",
            "Predicted Major Category: 13\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill relating to tax treatment of qualified dividend reinvestment plans.\n",
            "Original Major Category: 15\n",
            "Predicted Major Category: 1\n",
            "----------------------------------------------------------------------\n",
            "Original Title: A bill to remove residency requirements and acreage limitations applicable to land subject to reclamation law.\n",
            "Original Major Category: 21\n",
            "Predicted Major Category: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c392242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d61076-ef2e-4eaf-bcb4-4b901ed89778"
      },
      "source": [
        "model_path = 'transformer_model.pt'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to transformer_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf9978c4"
      },
      "source": [
        "We also built, trained, and evaluated a transformer model for minor category prediction, with an adjusted output layer with more dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19e9c81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b7cf97-cb09-450a-afe5-4003b84c8aef"
      },
      "source": [
        "encoded_labels_minor = np.array([labelsMinor.iloc[ii] for ii in non_zero_idx])\n",
        "\n",
        "train_y_minor = encoded_labels_minor[:split_idx]\n",
        "remaining_y_minor = encoded_labels_minor[split_idx:]\n",
        "\n",
        "val_y_minor = remaining_y_minor[:test_idx]\n",
        "test_y_minor = remaining_y_minor[test_idx:]\n",
        "\n",
        "print(\"Minor Label Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_y_minor.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_y_minor.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_y_minor.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minor Label Shapes:\n",
            "Train set: \t\t(47805,) \n",
            "Validation set: \t(5976,) \n",
            "Test set: \t\t(5976,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0db95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b35d6e-5dcd-4b86-b5a4-d5ea4091a787"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Determine the total number of unique minor categories from encoded_labels_minor\n",
        "# The instruction specifies to add 1 because the labels are 1-indexed, and to use np.max.\n",
        "# This assumes the maximum label value directly corresponds to the number of classes when 0-indexed.\n",
        "# If the minor labels were sequential 1, 2, ..., N, then int(np.max(labels)) would be N.\n",
        "# Since they are codes (e.g., 301, 503), the model will learn to predict logits for a sparse range up to max_minor_code+1.\n",
        "num_minor_classes = int(np.max(encoded_labels_minor)) + 1\n",
        "print(f\"Number of minor classes: {num_minor_classes}\")\n",
        "\n",
        "# 2. Create a new instance of the TransformerEncoderModel for minor categories\n",
        "# Use the previously defined vocab_size, embed_dim, num_heads, dense_dim, and seq_length\n",
        "minor_model = TransformerEncoderModel(\n",
        "    vocab_size,\n",
        "    embed_dim,\n",
        "    num_heads,\n",
        "    dense_dim,\n",
        "    seq_length,\n",
        "    num_minor_classes\n",
        ")\n",
        "\n",
        "# 3. Print the minor_model to inspect its architecture\n",
        "print(minor_model)\n",
        "\n",
        "# 4. Define the loss function for minor_model\n",
        "minor_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5. Define the optimizer for minor_model\n",
        "minor_optimizer = optim.Adam(minor_model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of minor classes: 10000\n",
            "TransformerEncoderModel(\n",
            "  (embedding): PositionalEmbedding(\n",
            "    (token_embeddings): Embedding(21048, 512, padding_idx=0)\n",
            "    (position_embeddings): Embedding(50, 512)\n",
            "  )\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (attention): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "    )\n",
            "    (dense_proj): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    )\n",
            "    (layernorm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (layernorm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=10000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64e27056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb14e36-2c27-4af5-db4a-f7f2ad8d1510",
        "collapsed": true
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create Tensor datasets for minor categories\n",
        "train_data_minor = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y_minor))\n",
        "valid_data_minor = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y_minor))\n",
        "\n",
        "# Dataloaders for minor categories\n",
        "batch_size = 64\n",
        "\n",
        "train_loader_minor = DataLoader(train_data_minor, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader_minor = DataLoader(valid_data_minor, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "print(\"Minor category data loaders created.\")\n",
        "\n",
        "# Train the minor category model\n",
        "def train_minor(model, train_loader, val_loader, num_epochs, train_on_gpu, criterion, optimizer):\n",
        "  # Move model to GPU if available\n",
        "  if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "      if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      # Adjust labels to be 0-indexed for CrossEntropyLoss (if they are 1-indexed)\n",
        "      labels = (labels - 1).long() # Assuming labels are 1-indexed\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "      ### Validation Loop ###\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in val_loader:\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        labels = (labels - 1).long() # Adjust labels for validation data too\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        val_acc = 100 * correct / total\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "train_minor(minor_model, train_loader_minor, valid_loader_minor, num_epochs, train_on_gpu, minor_criterion, minor_optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minor category data loaders created.\n",
            "Epoch 1/35, Training Loss: 3.377789859158104\n",
            "Epoch 1/35, Validation Accuracy: 41.68%\n",
            "Epoch 2/35, Training Loss: 1.7772896563878966\n",
            "Epoch 2/35, Validation Accuracy: 54.42%\n",
            "Epoch 3/35, Training Loss: 1.2042789970582037\n",
            "Epoch 3/35, Validation Accuracy: 61.16%\n",
            "Epoch 4/35, Training Loss: 0.9098488804322465\n",
            "Epoch 4/35, Validation Accuracy: 62.01%\n",
            "Epoch 5/35, Training Loss: 0.7730387424096345\n",
            "Epoch 5/35, Validation Accuracy: 64.05%\n",
            "Epoch 6/35, Training Loss: 0.6642481481541578\n",
            "Epoch 6/35, Validation Accuracy: 64.26%\n",
            "Epoch 7/35, Training Loss: 0.5734091485992833\n",
            "Epoch 7/35, Validation Accuracy: 65.36%\n",
            "Epoch 8/35, Training Loss: 0.5225439204287593\n",
            "Epoch 8/35, Validation Accuracy: 66.16%\n",
            "Epoch 9/35, Training Loss: 0.48494999662001714\n",
            "Epoch 9/35, Validation Accuracy: 66.57%\n",
            "Epoch 10/35, Training Loss: 0.4375516613770586\n",
            "Epoch 10/35, Validation Accuracy: 66.60%\n",
            "Epoch 11/35, Training Loss: 0.42812554054861096\n",
            "Epoch 11/35, Validation Accuracy: 66.60%\n",
            "Epoch 12/35, Training Loss: 0.40142001946713907\n",
            "Epoch 12/35, Validation Accuracy: 66.41%\n",
            "Epoch 13/35, Training Loss: 0.3784289263568359\n",
            "Epoch 13/35, Validation Accuracy: 67.00%\n",
            "Epoch 14/35, Training Loss: 0.3416773075432624\n",
            "Epoch 14/35, Validation Accuracy: 66.48%\n",
            "Epoch 15/35, Training Loss: 0.3515734999033625\n",
            "Epoch 15/35, Validation Accuracy: 67.51%\n",
            "Epoch 16/35, Training Loss: 0.3246363035974969\n",
            "Epoch 16/35, Validation Accuracy: 67.00%\n",
            "Epoch 17/35, Training Loss: 0.3447997093240633\n",
            "Epoch 17/35, Validation Accuracy: 66.92%\n",
            "Epoch 18/35, Training Loss: 0.32502578659327674\n",
            "Epoch 18/35, Validation Accuracy: 66.43%\n",
            "Epoch 19/35, Training Loss: 0.29376072171905726\n",
            "Epoch 19/35, Validation Accuracy: 67.09%\n",
            "Epoch 20/35, Training Loss: 0.2929221020147564\n",
            "Epoch 20/35, Validation Accuracy: 66.05%\n",
            "Epoch 21/35, Training Loss: 0.28508851336609903\n",
            "Epoch 21/35, Validation Accuracy: 68.16%\n",
            "Epoch 22/35, Training Loss: 0.26845323389780584\n",
            "Epoch 22/35, Validation Accuracy: 67.46%\n",
            "Epoch 23/35, Training Loss: 0.262239563708311\n",
            "Epoch 23/35, Validation Accuracy: 66.50%\n",
            "Epoch 24/35, Training Loss: 0.2722908163907621\n",
            "Epoch 24/35, Validation Accuracy: 66.68%\n",
            "Epoch 25/35, Training Loss: 0.26495178365935107\n",
            "Epoch 25/35, Validation Accuracy: 66.73%\n",
            "Epoch 26/35, Training Loss: 0.2523050817059248\n",
            "Epoch 26/35, Validation Accuracy: 66.26%\n",
            "Epoch 27/35, Training Loss: 0.24262360099393465\n",
            "Epoch 27/35, Validation Accuracy: 66.95%\n",
            "Epoch 28/35, Training Loss: 0.23517741768273848\n",
            "Epoch 28/35, Validation Accuracy: 66.78%\n",
            "Epoch 29/35, Training Loss: 0.23618231337497525\n",
            "Epoch 29/35, Validation Accuracy: 66.45%\n",
            "Epoch 30/35, Training Loss: 0.2203943661935927\n",
            "Epoch 30/35, Validation Accuracy: 67.27%\n",
            "Epoch 31/35, Training Loss: 0.216163389018959\n",
            "Epoch 31/35, Validation Accuracy: 66.75%\n",
            "Epoch 32/35, Training Loss: 0.23764434230985773\n",
            "Epoch 32/35, Validation Accuracy: 66.65%\n",
            "Epoch 33/35, Training Loss: 0.2613813316863999\n",
            "Epoch 33/35, Validation Accuracy: 66.15%\n",
            "Epoch 34/35, Training Loss: 0.22627009139283893\n",
            "Epoch 34/35, Validation Accuracy: 67.00%\n",
            "Epoch 35/35, Training Loss: 0.22344307124305107\n",
            "Epoch 35/35, Validation Accuracy: 66.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the validation accuracy for this model peaks around epoch 20, implying that the model begins to overfit soon after that. We also see that the overall accuracy is lower than that of the TF-IDF model. We did some optimization on this model, and while we are confident it is not perfect, we think that the TF-IDF model is the better choice."
      ],
      "metadata": {
        "id": "MyUJvtKhI9pM"
      }
    }
  ]
}